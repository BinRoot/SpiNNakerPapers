\documentclass[journal]{./sty/IEEEtran}

% *** CITATION PACKAGES ***
\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.


% *** MATH PACKAGES ***
%
\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
% \interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/


\usepackage{url}


% correct bad hyphenation here
\hyphenation{net-works SpiNNaker}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Modifying Synaptic Connections On The\\Spiking Neural Network Architecture (SpiNNaker)\\In Real-Time
}


\author{Matthew Frazier ~
        Nishant Shukla ~
        Worthy Martin* }

% The paper headers
\markboth{Journal of Neuroscience Methods,~Vol.~x, No.~y, January~2014}%
{Shell \MakeLowercase{\textit{et al.}}: Synaptogenesis on the SpiNNaker}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.


% make the title area
\maketitle

\begin{abstract}
Artificial Neural Networks is a promising approach to study human brain computation. Recent computer architecture design of a low-power 72-core processor by the University of Manchester (SpiNNaker) has made it easier to study highly parallel networks. We designed an algorithm on the SpiNNaker chip to enable synaptic removal, addition, and randomization on a neural network topology during run-time. Additionally, we explored scalability issues and unintended pitfalls with this approach.

\end{abstract}

\begin{IEEEkeywords}
Neural Networks, Synaptogenesis, SpiNNaker
\end{IEEEkeywords}


\section{Introduction}
\IEEEPARstart{T}{he} subfield of AI known as neural network computation has recently received a large and growing amount of attention. Put simply; neural networks (NN) are computational models inspired by biological brains that are capable of machine learning. A NN usually consists of interconnected "neurons" (in quotation marks for they are artificially simulated to varying degrees of biological accuracy -- but that topic is outside the scope of this paper) which compute from their inputs and produce various outputs, depending on the model. This paper focuses on types of spiking neural network (SNN) models, in which the neuronal communication (and therefore, the computation of the system as a whole) is achieved via message spikes, or action potentials, from one neuron that is synapsed onto another. In this introduction, we first describe how a SNN is a good computational representation of the brain. We then describe various spike-time learning methods used on a SNN, followed by an exploration of various types of SNN. We discuss why the neuromimetic hardware we use is a good platform for experimentation with SNN, and then describe our approach to the problem of simulating synaptogenesis on such a device and propose a solution. Our design section explores this solution in detail, and our implementation section describes our results.

( talk about Von Neuman architecture, and how the Turing machine does not encapsulate the entire class of computations. In fact, Hava Siegelmann proposed her ``Artificial Recurrent Neural Network'' could perform hypercomputation. )

\subsection{The Brain and other Spiking Neural Networks}
The brain is a spiking neural network. That is to say; biological neurons pass information around the brain via action potentials which travel from a neurons' soma (cell body) down the axon and generate synaptic events which are received by the dendrites of any neurons that the one in question has synapsed to. 
[Include figure illustrating this?]
The process has been simplified greatly as neurobiology is not the focus of this paper, nor is the biological process yet completely understood. What's important are the overarching properties exhibited by brains. Namely; that brains are fast, power-efficient, and capable of obtaining, representing, and integrating complex multi-dimensional information sets into useful knowledge (and from noisy sensory inputs, at that). Moreover, the property of being able to fairly reliably solve highly complex problems is something we want computers to have. 
[Include TSP example stating that human could do in a day what a procedural program would take hundreds of thousands of man-years to do?]
Progress is however being made towards the goal of making machines more like minds; indeed, the bus in a computers' hardware may operate at tens of MHz, while an axon may carry only tens to hundreds of action potentials per second, about five orders of magnitude slower than a machine (Furber 2007). The power efficiency of the brain is still far ahead of even the best neuromimetic hardware but the gap is closing quickly, considering that biological evolution had several hundred million years of a head start. One very important property of the brain that computers have a hard time with, which is also believed to be quintessential in mankind's ability to adapt to and solve new and ever-more difficult problems, is neural plasticity. It would be erroneous to say that no degree of plasticity has been implemented in any type of SNN, but given that we do not yet fully understand the mechanisms governing neural plasticity in our own heads, it suffices to say that there is yet much work to be done in this field of computational neuroscience in which artificial neural networks dynamically alter their own topology.

\subsection{Overview of Spike-time learning methods}
[TODO: Matt]
Before discussing different types of artificial spiking neural networks, it is necessary to describe various spike-time learning methods that are relevant to understanding how the different types of spiking neural networks function.
Spike time dependent plasticity (STDP) - 
Spike driven synaptic plasticity (SDSP) - 
Dynamic synaptic adjustments - 

\subsection{Types of Spiking Neural Networks}
[TODO: Matt]
There's SNN (default, outdated)
eSNN (evolves -alters synaptic weights- between a learning and recall phase)
deSNN (evolves dynamically - there's no differentiation between learning and recall (learns continuously))

\subsection{SpiNNaker}
[TODO: Matt]
The SpiNNaker is cool. 
Here's why. It's:
 -fast
 -power efficient
 -massively parallelized
 -capable of running (arbitrarily?) complex SNN sims
And we have one. 
And we added a cool functionality to it (maybe). 

\subsection{Synaptic Plasticity}
Other than updating synaptic weights to achieve learning, we introduce two additional forms of synaptic plasticity. 
In our study, we allow new synapses to be formed (synaptogenesis), or existing synapses to be entirely disconnected. The three types of plasticity undergo the following principles (Levy 2004):

\begin{itemize}
\item[(a)] {\bf Synaptogenesis} - 
Creation of new synapse depends on whether the postsynaptic neuron is at an optimal activation. If not, the formation of a new synapse is considered as an option to improve the network. Additionally, synaptogenesis only occurs between neighboring neurons. 

Synapses are formed by taking into account both receptivity and avidity (Levy 2004). Avidity is the measure of ablility for each neuron to participate is a new synaptic connection, defined as

\[
A_i(t)=\frac{a}{(a + \sum_j\sum_kW_{i_kj}(t))}
\]

\[
a = \left\{
\begin{array}{lr}
  1.0 * 10^{33} & \text{for unlimited avidity}\\
  1.0 & \text{for moderate avidity}\\
  1.0 * 10^{-3} & \text{for limiting avidity}
\end{array}
\right.
\]

The receptivity of new synaptic connections is inversly proportional to the running average of the neuron's activation. We use the following equation for receptivity 

\[
R_j(t) = \frac{r_1}{r_1 + \bar{y}_j(t)^{r_2}}
\]

\[
\bar{y}(t) = 0.99 \bar{y}_j(t-1) + 0.01 Y_j(t)
\]

with some experimental constants \(r_1\) and \(r_2\).

\[
\text{Prob}(\text{of new synapse } ij ) \propto A_i(t) * R_j(t)
\]

\item[(b)] {\bf Weight Modification} - 
Weights are adjusted following an unsupervised Hebbian rule. Specifically, we use the following rule:

\[
\triangle w(t+1) \propto f(\text{post}_j(t))*g(\text{pre}_j(t), w_{ij}(t))
\]

\item[(c)] {\bf Synapse Removal} -
Synaptic removal is a stoachastic process where the probability of removal is non-zero if excitatory synapses fall bellow a specified threshold. Given some constant \(\delta > 0\), following rule is used:

\[
\text{Prob}(\text{removal } ij) = \left\{
     \begin{array}{lr}
       0 & \text{if } w_{ij}(t) + \triangle w_{ij}(t+1) > \delta\\
       >0 & \text{otherwise}
     \end{array}
   \right.
\]

\end{itemize}

Synaptic connections are slowly modified through additions or removals until optimal output firing levels are obtained. 
Levy asserts that  ``information theory has gained popularity in recent years as a tool for understanding brain recordings.''


Define Synaptogenesis and explain why it's cool. (And why this model's different from deSNN -- entropy)
Here's why. 
Explain it's previous success from Levy's work.

\hfill

[TODO: Matt] We're combining the two and extending the deSNN model into an edeSNN model, which will shortly be enslaving the entire human race.


\section{Design}
\IEEEPARstart{N}{eural} networks consist of biologically inspired relationships between individual nodes (neurons). Learning in such a network occurs by intelligently adjusting weights on the links between the nodes. We define the relationship between nodes into the following three categories:

\begin{itemize}
\item[(a)] {\bf Static} - where the network topology is fixed, and can only change by manually adjusting the relationship between nodes. Most neural networks fall into this category because regardless of the number of nodes \(n\), only one fixed relationship forms between them:

\[
\text{NumberOfTopologies}(n) = 1
\]


\item[(b)] {\bf Semi-Dynamic} - in which the network is not static, and there is some freedom for nodes to rewire with other nodes in real time. The complexity grows exponentially. Each node has non-zero probability to be rewired with a constant \(c\) number of other nodes. Given \(n\) such nodes, the number of possible topologies can be up to

\[
\text{NumberOfTopologies}(n) = c^n
\]


\item[(c)] {\bf Dynamic} - where each node has non-zero probability to be wired with any other neuron in the entire network. Given \(n\) nodes, the number of possible topologies becomes

\[
\text{NumberOfTopologies}(n) = n^{n-1}
\]


\end{itemize}

A biological neural network such as the human brain does not follow the static network topology. Connections between neurons are regularly formed and removed over time. Moreover, such a neural network is not fully dynamic either, since locality is a physical constraint. For example, a neuron on the far end of the left hemisphere of a brain may never directly connect with a neuron on the right hemisphere.

We designed a programming framework on SpiNNaker's API to enable a semi-dynamic network topology. In our implementation, each neuron had the flexibility to rewire with none, all, or some of the fixed number of other neurons.

\section{Implementation}
\IEEEPARstart{B}{y} taking advantage of the multiple independent cores on the SpiNNaker, we were able to emulate efficient semi-dynamic synaptogenesis. We implemented a perceptron with fifteen nodes consisting of nine input layers and six output layers. As diagrammed in Figure 1 below, each of the nine neurons broadcasted its excitation to an arbitrarily chosen six output neurons. 

\vspace{0.5cm}
[Figure 1]
\vspace{0.5cm}

We used the weight modification rule proposed by Levy to categorize an input set of letters. 

\vspace{0.5cm}
[Delta W rule]
\vspace{0.5cm}

We implemented a single-layer perception as a proof of concept to demonstrate synaptogenesis on the SpiNNaker board. In our network, every input node broadcasts a data packet to six of the output nodes. To simulate the creation and rewiring of synapses, not all messages are registered by the output neurons. 

Each packet received by an output node is looked up in the output node's local list of incoming connections. If the address from the packet is not listed in the incoming connections list, it will be ignored, and no further computation will be done.

Synaptic connectivity is broken when weights fall below some negative threshold. More specifically, when an average running weight dropped below \(-\delta\) for some \(\delta > 0\), the link is considered disconnected.

New connections are established between nodes when an average rate of activity drops below 25\%. In this case, the next data packet received from a muted neuron becomes unmuted, in hopes to raise the average rate of activity.

\section{Pitfalls}
\IEEEPARstart{S}{ome} disadvantages are present when considering our approach for synaptogenesis on the SpiNNaker. [TODO: Matt] Blah blah.


\section{Further Study}
\IEEEPARstart{T}{his} paper reveals multiple questions that still need to be examined further.

\section{Conclusion}
The conclusion goes here.


% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


\appendices
\section{Shannon's Entropy}
Appendix one text goes here.

\section{Calculation of Mutual Information}
Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
\section{Calculation of Statistical Dependence}
Appendix two text goes here.


% use section* for acknowledgement
\section*{Acknowledgment}


The authors would like to thank Professor Worthy Martin, Associate Professor of Computer Science at the University of Virginia.


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

\bibitem{IEEEhowto:kopka}
H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

\end{thebibliography}

\end{document}


